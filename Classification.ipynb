{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM4Vxda6Bwliw3YBnKdii+k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"KbfywfVSdGOR","colab_type":"code","outputId":"6562bf2c-c435-4f3e-aabd-21875d12ceba","executionInfo":{"status":"ok","timestamp":1589108843100,"user_tz":-120,"elapsed":24360,"user":{"displayName":"vu ngoc minh on","photoUrl":"","userId":"08489535657617177327"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","import os\n","\n","drive.mount(\"/content/gdrive\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TFz-ga3wdy73","colab_type":"code","outputId":"689c1b1f-bbf8-4566-a3a6-58e3b4000c10","executionInfo":{"status":"ok","timestamp":1589112941671,"user_tz":-120,"elapsed":3103,"user":{"displayName":"vu ngoc minh on","photoUrl":"","userId":"08489535657617177327"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!ls"],"execution_count":6,"outputs":[{"output_type":"stream","text":["8k_normal_vs_camouflage_clothes_images\tkaggle.json\n","build_dataset.py\t\t\tplot.png\n","camo_detector.model\t\t\tpyimagesearch\n","Classification.ipynb\t\t\ttrain_camo_detector.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tiJPDl0Wd1Jx","colab_type":"code","outputId":"6b52e1ed-c23a-4cf9-bf9a-d097bd879fe2","executionInfo":{"status":"ok","timestamp":1589108851375,"user_tz":-120,"elapsed":508,"user":{"displayName":"vu ngoc minh on","photoUrl":"","userId":"08489535657617177327"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%cd '/content/gdrive/My Drive/deep_learning/Classification'"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/deep_learning/Classification\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Of8LQvhL4O4s","colab_type":"code","colab":{}},"source":["from google.colab import files\n","files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"94mJmIHU4wnT","colab_type":"code","colab":{}},"source":["!pip install -q kaggle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fFuQuHtm5GkZ","colab_type":"code","colab":{}},"source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTzA2-0h5kvU","colab_type":"code","colab":{}},"source":["!kaggle datasets list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U8j1tw6H82M4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uqZsunvceglz","colab_type":"code","colab":{}},"source":["!kaggle datasets download -d imneonizer/normal-vs-camouflage-clothes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"72ylCGw37hSu","colab_type":"code","colab":{}},"source":["!unzip normal-vs-camouflage-clothes.zip\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dIg9yKJp83HY","colab_type":"code","outputId":"ac5bcae8-e9fb-4ba5-fbb5-e783b18d4957","executionInfo":{"status":"ok","timestamp":1589120354185,"user_tz":-120,"elapsed":7399787,"user":{"displayName":"vu ngoc minh on","photoUrl":"","userId":"08489535657617177327"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["# USAGE\n","# python build_dataset.py\n","\n","# import the necessary packages\n","from pyimagesearch import config\n","from imutils import paths\n","import random\n","import shutil\n","import os\n","\n","# grab the paths to all input images in the original input directory\n","# and shuffle them\n","imagePaths = list(paths.list_images(config.ORIG_INPUT_DATASET))\n","random.seed(42)\n","random.shuffle(imagePaths)\n","\n","# compute the training and testing split\n","i = int(len(imagePaths) * config.TRAIN_SPLIT)\n","trainPaths = imagePaths[:i]\n","testPaths = imagePaths[i:]\n","\n","# we'll be using part of the training data for validation\n","i = int(len(trainPaths) * config.VAL_SPLIT)\n","valPaths = trainPaths[:i]\n","trainPaths = trainPaths[i:]\n","\n","# define the datasets that we'll be building\n","datasets = [\n","\t(\"training\", trainPaths, config.TRAIN_PATH),\n","\t(\"validation\", valPaths, config.VAL_PATH),\n","\t(\"testing\", testPaths, config.TEST_PATH)\n","]\n","\n","# loop over the datasets\n","for (dType, imagePaths, baseOutput) in datasets:\n","\t# show which data split we are creating\n","\tprint(\"[INFO] building '{}' split\".format(dType))\n","\n","\t# if the output base output directory does not exist, create it\n","\tif not os.path.exists(baseOutput):\n","\t\tprint(\"[INFO] 'creating {}' directory\".format(baseOutput))\n","\t\tos.makedirs(baseOutput)\n","\n","\t# loop over the input image paths\n","\tfor inputPath in imagePaths:\n","\t\t# extract the filename of the input image along with its\n","\t\t# corresponding class label\n","\t\tfilename = inputPath.split(os.path.sep)[-1]\n","\t\tlabel = inputPath.split(os.path.sep)[-2]\n","\n","\t\t# build the path to the label directory\n","\t\tlabelPath = os.path.sep.join([baseOutput, label])\n","\n","\t\t# if the label output directory does not exist, create it\n","\t\tif not os.path.exists(labelPath):\n","\t\t\tprint(\"[INFO] 'creating {}' directory\".format(labelPath))\n","\t\t\tos.makedirs(labelPath)\n","\n","\t\t# construct the path to the destination image and then copy\n","\t\t# the image itself\n","\t\tp = os.path.sep.join([labelPath, filename])\n","\t\tshutil.copy2(inputPath, p)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[INFO] building 'training' split\n","[INFO] 'creating camo_not_camo/training' directory\n","[INFO] 'creating camo_not_camo/training/camouflage_clothes' directory\n","[INFO] 'creating camo_not_camo/training/normal_clothes' directory\n","[INFO] building 'validation' split\n","[INFO] 'creating camo_not_camo/validation' directory\n","[INFO] 'creating camo_not_camo/validation/normal_clothes' directory\n","[INFO] 'creating camo_not_camo/validation/camouflage_clothes' directory\n","[INFO] building 'testing' split\n","[INFO] 'creating camo_not_camo/testing' directory\n","[INFO] 'creating camo_not_camo/testing/camouflage_clothes' directory\n","[INFO] 'creating camo_not_camo/testing/normal_clothes' directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B7h5qtUXAluS","colab_type":"code","outputId":"95b60980-d4ce-472d-afcc-72d0a49a29c0","executionInfo":{"status":"ok","timestamp":1589126435771,"user_tz":-120,"elapsed":5930701,"user":{"displayName":"vu ngoc minh on","photoUrl":"","userId":"08489535657617177327"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# USAGE\n","# python train_camo_detector.py\n","\n","# set the matplotlib backend so figures can be saved in the background\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","\n","# import the necessary packages\n","from pyimagesearch import config\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import AveragePooling2D\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications import ResNet50\n","from sklearn.metrics import classification_report\n","from imutils import paths\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import argparse\n","\n","# construct the argument parser and parse the arguments\n","# ap = argparse.ArgumentParser()\n","# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n","# \thelp=\"path to output loss/accuracy plot\")\n","# args = vars(ap.parse_args())\n","\n","# determine the total number of image paths in training, validation,\n","# and testing directories\n","totalTrain = len(list(paths.list_images(config.TRAIN_PATH)))\n","totalVal = len(list(paths.list_images(config.VAL_PATH)))\n","totalTest = len(list(paths.list_images(config.TEST_PATH)))\n","\n","# initialize the training training data augmentation object\n","trainAug = ImageDataGenerator(\n","\trotation_range=25,\n","\tzoom_range=0.1,\n","\twidth_shift_range=0.1,\n","\theight_shift_range=0.1,\n","\tshear_range=0.2,\n","\thorizontal_flip=True,\n","\tfill_mode=\"nearest\")\n","\n","# initialize the validation/testing data augmentation object (which\n","# we'll be adding mean subtraction to)\n","valAug = ImageDataGenerator()\n","\n","# define the ImageNet mean subtraction (in RGB order) and set the\n","# the mean subtraction value for each of the data augmentation\n","# objects\n","mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n","trainAug.mean = mean\n","valAug.mean = mean\n","\n","# initialize the training generator\n","trainGen = trainAug.flow_from_directory(\n","\tconfig.TRAIN_PATH,\n","\tclass_mode=\"categorical\",\n","\ttarget_size=(224, 224),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=True,\n","\tbatch_size=config.BS)\n","\n","# initialize the validation generator\n","valGen = valAug.flow_from_directory(\n","\tconfig.VAL_PATH,\n","\tclass_mode=\"categorical\",\n","\ttarget_size=(224, 224),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=False,\n","\tbatch_size=config.BS)\n","\n","# initialize the testing generator\n","testGen = valAug.flow_from_directory(\n","\tconfig.TEST_PATH,\n","\tclass_mode=\"categorical\",\n","\ttarget_size=(224, 224),\n","\tcolor_mode=\"rgb\",\n","\tshuffle=False,\n","\tbatch_size=config.BS)\n","\n","# load the ResNet-50 network, ensuring the head FC layer sets are left\n","# off\n","print(\"[INFO] preparing model...\")\n","baseModel = ResNet50(weights=\"imagenet\", include_top=False,\n","\tinput_tensor=Input(shape=(224, 224, 3)))\n","\n","# construct the head of the model that will be placed on top of the\n","# the base model\n","headModel = baseModel.output\n","headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n","headModel = Flatten(name=\"flatten\")(headModel)\n","headModel = Dense(256, activation=\"relu\")(headModel)\n","headModel = Dropout(0.5)(headModel)\n","headModel = Dense(len(config.CLASSES), activation=\"softmax\")(headModel)\n","\n","# place the head FC model on top of the base model (this will become\n","# the actual model we will train)\n","model = Model(inputs=baseModel.input, outputs=headModel)\n","\n","# loop over all layers in the base model and freeze them so they will\n","# *not* be updated during the training process\n","for layer in baseModel.layers:\n","\tlayer.trainable = False\n","\n","# compile the model\n","opt = Adam(lr=config.INIT_LR, decay=config.INIT_LR / config.NUM_EPOCHS)\n","model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n","\tmetrics=[\"accuracy\"])\n","\n","# train the model\n","print(\"[INFO] training model...\")\n","H = model.fit_generator(\n","\ttrainGen,\n","\tsteps_per_epoch=totalTrain // config.BS,\n","\tvalidation_data=valGen,\n","\tvalidation_steps=totalVal // config.BS,\n","\tepochs=config.NUM_EPOCHS)\n","\n","# reset the testing generator and then use our trained model to\n","# make predictions on the data\n","print(\"[INFO] evaluating network...\")\n","testGen.reset()\n","predIdxs = model.predict_generator(testGen,\n","\tsteps=(totalTest // config.BS) + 1)\n","\n","# for each image in the testing set we need to find the index of the\n","# label with corresponding largest predicted probability\n","predIdxs = np.argmax(predIdxs, axis=1)\n","\n","# show a nicely formatted classification report\n","print(classification_report(testGen.classes, predIdxs,\n","\ttarget_names=testGen.class_indices.keys()))\n","\n","# serialize the model to disk\n","print(\"[INFO] saving model...\")\n","model.save(config.MODEL_PATH, save_format=\"h5\")\n","\n","# plot the training loss and accuracy\n","N = config.NUM_EPOCHS\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n","plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n","plt.title(\"Training Loss and Accuracy on Dataset\")\n","plt.xlabel(\"Epoch #\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"lower left\")\n","plt.savefig(\"plot.png\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Found 10731 images belonging to 2 classes.\n","Found 1192 images belonging to 2 classes.\n","Found 3975 images belonging to 2 classes.\n","[INFO] preparing model...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94773248/94765736 [==============================] - 1s 0us/step\n","[INFO] training model...\n","WARNING:tensorflow:From <ipython-input-8-147d96c0ffb7>:117: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.fit, which supports generators.\n","Epoch 1/20\n","335/335 [==============================] - 310s 927ms/step - loss: 0.1772 - accuracy: 0.9309 - val_loss: 0.0882 - val_accuracy: 0.9721\n","Epoch 2/20\n","335/335 [==============================] - 298s 891ms/step - loss: 0.0841 - accuracy: 0.9703 - val_loss: 0.0680 - val_accuracy: 0.9789\n","Epoch 3/20\n","335/335 [==============================] - 296s 885ms/step - loss: 0.0669 - accuracy: 0.9752 - val_loss: 0.0708 - val_accuracy: 0.9789\n","Epoch 4/20\n","335/335 [==============================] - 295s 880ms/step - loss: 0.0573 - accuracy: 0.9788 - val_loss: 0.0650 - val_accuracy: 0.9814\n","Epoch 5/20\n","335/335 [==============================] - 294s 877ms/step - loss: 0.0530 - accuracy: 0.9807 - val_loss: 0.0660 - val_accuracy: 0.9797\n","Epoch 6/20\n","335/335 [==============================] - 293s 875ms/step - loss: 0.0446 - accuracy: 0.9834 - val_loss: 0.0612 - val_accuracy: 0.9823\n","Epoch 7/20\n","335/335 [==============================] - 294s 877ms/step - loss: 0.0422 - accuracy: 0.9861 - val_loss: 0.0635 - val_accuracy: 0.9831\n","Epoch 8/20\n","335/335 [==============================] - 293s 875ms/step - loss: 0.0374 - accuracy: 0.9862 - val_loss: 0.0732 - val_accuracy: 0.9755\n","Epoch 9/20\n","335/335 [==============================] - 291s 867ms/step - loss: 0.0369 - accuracy: 0.9872 - val_loss: 0.0609 - val_accuracy: 0.9823\n","Epoch 10/20\n","335/335 [==============================] - 289s 861ms/step - loss: 0.0339 - accuracy: 0.9887 - val_loss: 0.0592 - val_accuracy: 0.9840\n","Epoch 11/20\n","335/335 [==============================] - 290s 867ms/step - loss: 0.0319 - accuracy: 0.9883 - val_loss: 0.0604 - val_accuracy: 0.9806\n","Epoch 12/20\n","335/335 [==============================] - 290s 864ms/step - loss: 0.0287 - accuracy: 0.9893 - val_loss: 0.0650 - val_accuracy: 0.9831\n","Epoch 13/20\n","335/335 [==============================] - 288s 859ms/step - loss: 0.0288 - accuracy: 0.9896 - val_loss: 0.0662 - val_accuracy: 0.9797\n","Epoch 14/20\n","335/335 [==============================] - 288s 858ms/step - loss: 0.0262 - accuracy: 0.9907 - val_loss: 0.0903 - val_accuracy: 0.9747\n","Epoch 15/20\n","335/335 [==============================] - 287s 857ms/step - loss: 0.0274 - accuracy: 0.9897 - val_loss: 0.0697 - val_accuracy: 0.9814\n","Epoch 16/20\n","335/335 [==============================] - 285s 852ms/step - loss: 0.0241 - accuracy: 0.9916 - val_loss: 0.0660 - val_accuracy: 0.9806\n","Epoch 17/20\n","335/335 [==============================] - 288s 859ms/step - loss: 0.0250 - accuracy: 0.9907 - val_loss: 0.0568 - val_accuracy: 0.9831\n","Epoch 18/20\n","335/335 [==============================] - 287s 856ms/step - loss: 0.0224 - accuracy: 0.9921 - val_loss: 0.0561 - val_accuracy: 0.9848\n","Epoch 19/20\n","335/335 [==============================] - 288s 861ms/step - loss: 0.0211 - accuracy: 0.9921 - val_loss: 0.0581 - val_accuracy: 0.9831\n","Epoch 20/20\n","335/335 [==============================] - 288s 859ms/step - loss: 0.0218 - accuracy: 0.9921 - val_loss: 0.0631 - val_accuracy: 0.9797\n","[INFO] evaluating network...\n","WARNING:tensorflow:From <ipython-input-8-147d96c0ffb7>:124: Model.predict_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.predict, which supports generators.\n","                    precision    recall  f1-score   support\n","\n","camouflage_clothes       0.99      0.99      0.99      1968\n","    normal_clothes       0.99      0.99      0.99      2007\n","\n","          accuracy                           0.99      3975\n","         macro avg       0.99      0.99      0.99      3975\n","      weighted avg       0.99      0.99      0.99      3975\n","\n","[INFO] saving model...\n"],"name":"stdout"}]}]}